{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#800 node vs 400\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "tf.disable_eager_execution()\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply, Reshape\n",
    "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda, Embedding,Dropout,Add,Softmax,GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2, l1\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadmodel=0\n",
    "forceteach=0\n",
    "orignial=0\n",
    "pretrain_fr=1\n",
    "attention='Loung' #'Bahdanau'\n",
    "layerofencoderlstm=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##NEEDS DROP OUT\n",
    "\n",
    "\n",
    "#french pretrained embedding\n",
    "#encoderstate to decoder \n",
    "#forceteach\n",
    "#Bahdanau \n",
    "#attention='Bahdanau' #loungAttention\n",
    "unkcntlimit=1\n",
    "uniquewordscnt=4000\n",
    "\n",
    "#tanh vs none\n",
    "#vocabRatio=0.89\n",
    "\n",
    "\n",
    "rnn_size=400\n",
    "activationF='tanh'\n",
    "loungAttentionNodeCnt=rnn_size\n",
    "lstmregulizer=None\n",
    "denseregulizer=None\n",
    "\n",
    "wordvectorsize=100\n",
    "\n",
    "maxlength=30\n",
    "lstmdropout=0.1\n",
    "recur_dropout=0.1\n",
    "betweenlayerdropout=0.5\n",
    "convertoworddropout=0.5\n",
    "\n",
    "decoderlstmdropout=0.1\n",
    "decoderrecurdropout=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English source data\n",
    "if (orignial==1):\n",
    "    with open(\"./data/small_vocab_en\", \"r\", encoding=\"utf-8\") as f:\n",
    "        source_text = f.read()\n",
    "\n",
    "    # French target data\n",
    "    with open(\"./data/small_vocab_fr\", \"r\", encoding=\"utf-8\") as f:\n",
    "        target_text = f.read()\n",
    "else:\n",
    "    \n",
    "    with open(\"../data/small_vocab_en_UN\", \"r\", encoding=\"utf-8\") as f:\n",
    "        source_text = f.read()\n",
    "\n",
    "    # French target data\n",
    "    with open(\"../data/small_vocab_fr_UN\", \"r\", encoding=\"utf-8\") as f:\n",
    "        target_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123279247\n",
      "136885361\n",
      "Dataset Stats\n",
      "Roughly the number of unique words: 76939\n",
      "-----English Text-----\n",
      "Number of sentences: 800001\n",
      "Average number of words in a sentence: 25.53115308605864\n",
      "Max number of words in a sentence: 1449\n",
      "\n",
      "-----French Text-----\n",
      "Number of sentences: 800001\n",
      "Average number of words in a sentence: 27.901697622877972\n",
      "Max number of words in a sentence: 1607\n",
      "\n",
      "English sentences 0 to 20:\n",
      "UNITED NATIONS\n",
      "E\n",
      "Economic and Social Council\n",
      "Distr .\n",
      "GENERAL\n",
      " February \n",
      "Original :  FRENCH\n",
      "ECONOMIC COMMISSION FOR EUROPE\n",
      "INLAND TRANSPORT COMMITTEE\n",
      "Working Party on the Construction of Vehicles\n",
      "One hundred and seventeenth session ,\n",
      "  March  , agenda item  .\n",
      "PROPOSAL FOR DRAFT SUPPLEMENT  TO REGULATION No . \n",
      "Vehicle alarm systems\n",
      "Addendum \n",
      "Transmitted by the expert from the International Organization of\n",
      "Motor Vehicle Manufacturers OICA\n",
      "Note :  The text reproduced below was prepared by the expert from OICA in accordance with the decision of GRSG at its seventy fifth session TRANS/WP ./GRSG/ .\n",
      "GE .  E page\n",
      "Title of document , amend to read : \n",
      "\n",
      "French sentences 0 to 20:\n",
      "NATIONS\n",
      "E\n",
      "Conseil Économique\n",
      "Distr .\n",
      "GÉNÉRALE\n",
      " février \n",
      "Original  :  FRANÇAIS\n",
      "COMMISSION ÉCONOMIQUE POUR L'EUROPE\n",
      "COMITÉ DES TRANSPORTS INTÉRIEURS\n",
      "Groupe de travail de la construction des véhicules\n",
      "Cent—dix septième session , — mars  ,\n",
      "point  . de l'ordre du jour\n",
      "PROPOSITION DE COMPLÉMENT  AU RÈGLEMENT No \n",
      "Systèmes d’alarme des véhicules\n",
      "Additif \n",
      "Transmis par l’expert de l’Organisation internationale des\n",
      "Constructeurs d’automobiles OICA\n",
      "Note  :  Le texte reproduit ci—après a été établi par l’expert de l’OICA suivant le décision du GRSG à sa soixante quinzième session TRANS/WP ./GRSG/ ,\n",
      "par .  .\n",
      "Titre du document , modifier comme suit  : \n"
     ]
    }
   ],
   "source": [
    "\n",
    "newstring=[]\n",
    "skipmod=0\n",
    "for i in range(len(source_text)*2):\n",
    "    try:\n",
    "        char=source_text[i]\n",
    "    except:\n",
    "        print(i)\n",
    "        break\n",
    "    if(char==' ' or char==\"\\\\\" ):\n",
    "        skipmod=0\n",
    "    if(skipmod==1):\n",
    "        continue\n",
    "\n",
    "    if(char.isnumeric()==True):\n",
    "        continue\n",
    "    elif(char==')'):\n",
    "        continue\n",
    "    elif(char=='('):\n",
    "        continue\n",
    "    elif(char=='\"'):\n",
    "        continue\n",
    "    elif(char=='['):\n",
    "        continue\n",
    "    elif(char==']'):\n",
    "        continue\n",
    "    elif(char==';'):\n",
    "        continue\n",
    "    elif(char=='-'):\n",
    "        newstring.append(' ')\n",
    "        continue        \n",
    "    elif(char==':'):\n",
    "        newstring.append(' ')\n",
    "        newstring.append(':')\n",
    "        newstring.append(' ')\n",
    "        continue    \n",
    "\n",
    "  #  elif(char=='.' and source_text[i-1].isnumeric()==True):\n",
    "  #      continue\n",
    "    \n",
    "    elif(char==',' and source_text[i-1]!=' '):\n",
    "        newstring.append(' ')\n",
    "        newstring.append(',')\n",
    "     \n",
    "    elif(char=='.' and source_text[i-1]!=' '):\n",
    "        newstring.append(' ')\n",
    "        newstring.append('.')\n",
    "        \n",
    "    else:\n",
    "        newstring.append(char)\n",
    "    \n",
    "newstring=''.join(newstring)    \n",
    "del source_text\n",
    "source_text=newstring\n",
    "       \n",
    "newstring2=[]\n",
    "skipmod=0\n",
    "for i in range(len(target_text)*2):\n",
    "    try:\n",
    "        char=target_text[i]\n",
    "    except:\n",
    "        print(i)\n",
    "        break\n",
    "    if(char==' ' or '\\\\'):\n",
    "        skipmod=0\n",
    "    if(skipmod==1):\n",
    "        continue\n",
    "\n",
    "    if(char.isnumeric()==True):\n",
    "        continue\n",
    "    elif(char==')'):\n",
    "        continue\n",
    "    elif(char=='('):\n",
    "        continue\n",
    "    elif(char=='\"'):\n",
    "        continue\n",
    "    elif(char=='['):\n",
    "        continue\n",
    "    elif(char==']'):\n",
    "        continue\n",
    "    elif(char==';'):\n",
    "        continue\n",
    "    elif(char=='-'):\n",
    "        newstring2.append(' ')\n",
    "    elif(char==':'):\n",
    "        newstring2.append(' ')\n",
    "        newstring2.append(':')\n",
    "        newstring2.append(' ')\n",
    "        continue    \n",
    "\n",
    "   # elif(char=='.' and target_text[i-1].isnumeric()==True):\n",
    "   #     continue\n",
    "    \n",
    "    elif(char==',' and target_text[i-1]!=' '):\n",
    "        newstring2.append(' ')\n",
    "        newstring2.append(',')\n",
    "      \n",
    "    elif(char=='.' and target_text[i-1]!=' '):\n",
    "        newstring2.append(' ')\n",
    "        newstring2.append('.')\n",
    "      \n",
    "    else:\n",
    "        newstring2.append(char)\n",
    "    \n",
    "newstring2=''.join(newstring2)   \n",
    "del target_text\n",
    "target_text=newstring2\n",
    "\n",
    "view_sentence_range = (0, 20)\n",
    "\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word.lower(): None for word in source_text.split()})))\n",
    "\n",
    "\n",
    "print(\"-\"*5 + \"English Text\" + \"-\"*5)\n",
    "sentences = source_text.split('\\n')\n",
    "word_counts = [len(sentence.split()) for sentence in sentences]\n",
    "print('Number of sentences: {}'.format(len(sentences)))\n",
    "print('Average number of words in a sentence: {}'.format(np.average(word_counts)))\n",
    "print('Max number of words in a sentence: {}'.format(np.max(word_counts)))\n",
    "\n",
    "print()\n",
    "print(\"-\"*5 + \"French Text\" + \"-\"*5)\n",
    "sentences = target_text.split('\\n')\n",
    "word_counts = [len(sentence.split()) for sentence in sentences]\n",
    "print('Number of sentences: {}'.format(len(sentences)))\n",
    "print('Average number of words in a sentence: {}'.format(np.average(word_counts)))\n",
    "print('Max number of words in a sentence: {}'.format(np.max(word_counts)))\n",
    "\n",
    "print()\n",
    "print('English sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(source_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))\n",
    "print()\n",
    "print('French sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(target_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_textlist=source_text.splitlines(keepends=True)\n",
    "target_textlist=target_text.splitlines(keepends=True)\n",
    "#delete long lines and lines with too many words not alphbet\n",
    "i=-1\n",
    "\n",
    "while(i<len(source_textlist)):\n",
    "    i+=1\n",
    "    if(i%1000==0):\n",
    "        print(i)\n",
    "    try:\n",
    "        line=source_textlist[i]\n",
    "    except:\n",
    "        print(i)\n",
    "        break  \n",
    "    wordcnt=0\n",
    "    notalphbetcnt=0\n",
    "    line=line.split()\n",
    "    for word in line:\n",
    "        wordcnt+=1\n",
    "        if(word.isalpha()== False):\n",
    "            notalphbetcnt+=1\n",
    "    \n",
    "    if(wordcnt>29):\n",
    "        \n",
    "        del source_textlist[i]\n",
    "        del target_textlist[i]\n",
    "        i-=1\n",
    "    elif(wordcnt<3):\n",
    "        \n",
    "        del source_textlist[i]\n",
    "        del target_textlist[i]   \n",
    "        i-=1    \n",
    "    elif(notalphbetcnt/wordcnt>0.3):\n",
    "        \n",
    "        del source_textlist[i]\n",
    "        del target_textlist[i]   \n",
    "        i-=1    \n",
    "i=-1\n",
    "while(i<len(target_textlist)):\n",
    "    i+=1\n",
    "    if(i%1000==0):\n",
    "        print(i)\n",
    "    try:\n",
    "        line=target_textlist[i]\n",
    "    except:\n",
    "        print(i)\n",
    "        break  \n",
    "    wordcnt=0\n",
    "    notalphbetcnt=0\n",
    "    line=line.split()\n",
    "    for word in line:\n",
    "        wordcnt+=1\n",
    "\n",
    "    \n",
    "    if(wordcnt>29):\n",
    "        \n",
    "        del source_textlist[i]\n",
    "        del target_textlist[i]\n",
    "        i-=1\n",
    "\n",
    "\n",
    "print(len(source_textlist))\n",
    "print(len(target_textlist))\n",
    "\n",
    "source_text=''.join(source_textlist)\n",
    "target_text=''.join(target_textlist)\n",
    "del source_textlist\n",
    "del target_textlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----English example-----\n",
      "a . Access to financial resources for sustainable development\n",
      "\n",
      "-----French example-----\n",
      "a Accès aux ressources financières en vue du développement durable\n",
      "-----English example-----\n",
      "v Reports submitted by other intergovernmental organizations , for example , the Commonwealth Secretariat and the South Pacific Regional Environment Programme\n",
      "\n",
      "-----French example-----\n",
      "v Rapports présentés par d'autres organisations intergouvernementales , par exemple le Secrétariat du Commonwealth et le Programme régional pour l'environnement du Pacifique Sud\n"
     ]
    }
   ],
   "source": [
    "random_index = 30000\n",
    "\n",
    "print(\"-\"*5 + \"English example\" + \"-\"*5)\n",
    "print(source_text.split(\"\\n\")[random_index])\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"-\"*5 + \"French example\" + \"-\"*5)\n",
    "print(target_text.split(\"\\n\")[random_index])\n",
    "\n",
    "\n",
    "print(\"-\"*5 + \"English example\" + \"-\"*5)\n",
    "print(source_text.split(\"\\n\")[random_index+5])\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"-\"*5 + \"French example\" + \"-\"*5)\n",
    "print(target_text.split(\"\\n\")[random_index+5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 45980\n",
      "-----English Text-----\n",
      "Number of sentences: 370833\n",
      "Average number of words in a sentence: 14.75138943945118\n",
      "Max number of words in a sentence: 29\n",
      "\n",
      "-----French Text-----\n",
      "Number of sentences: 370833\n",
      "Average number of words in a sentence: 15.788228124249999\n",
      "Max number of words in a sentence: 29\n",
      "\n",
      "English sentences 0 to 20:\n",
      "Economic and Social Council\n",
      "ECONOMIC COMMISSION FOR EUROPE\n",
      "INLAND TRANSPORT COMMITTEE\n",
      "Working Party on the Construction of Vehicles\n",
      "One hundred and seventeenth session ,\n",
      "PROPOSAL FOR DRAFT SUPPLEMENT  TO REGULATION No . \n",
      "Vehicle alarm systems\n",
      "Transmitted by the expert from the International Organization of\n",
      "Motor Vehicle Manufacturers OICA\n",
      "Note :  The text reproduced below was prepared by the expert from OICA in accordance with the decision of GRSG at its seventy fifth session TRANS/WP ./GRSG/ .\n",
      "GE .  E page\n",
      "Title of document , amend to read : \n",
      "“PROPOSAL FOR THE  SERIES OF AMENDMENTS TO REGULATION No . ”\n",
      "“ . . . an approval number of which the first two digits currently  for the  series of amendments shall indicate . . .”\n",
      "Insert the following new paragraphs  to  . : \n",
      " . Approval of a vehicle type\n",
      "Paragraph  former , becomes paragraph \n",
      "In the example of approval marks and in the captions below , replace approval number “” by “” .\n",
      "The caption to model B should read : \n",
      "COMMENTS RECEIVED FROM MEMBER STATES . \n",
      "\n",
      "French sentences 0 to 20:\n",
      "Conseil Économique\n",
      "COMMISSION ÉCONOMIQUE POUR L'EUROPE\n",
      "COMITÉ DES TRANSPORTS INTÉRIEURS\n",
      "Groupe de travail de la construction des véhicules\n",
      "Cent—dix septième session , — mars  ,\n",
      "PROPOSITION DE COMPLÉMENT  AU RÈGLEMENT No \n",
      "Systèmes d’alarme des véhicules\n",
      "Transmis par l’expert de l’Organisation internationale des\n",
      "Constructeurs d’automobiles OICA\n",
      "Note  :  Le texte reproduit ci—après a été établi par l’expert de l’OICA suivant le décision du GRSG à sa soixante quinzième session TRANS/WP ./GRSG/ ,\n",
      "par .  .\n",
      "Titre du document , modifier comme suit  : \n",
      "“PROPOSITION DE LA SERIE  D’AMENDEMENTS AU REGLEMENT No \n",
      "“ . un numéro d’homologation dont les deux premiers chiffres actuellement  correspondant à la série  d’amendements indiquent .”\n",
      "Insérer les nouveaux paragraphes  . à  . . , ainsi libellés  : \n",
      "Homologation d’un type de véhicule\n",
      "Le paragraphe  ancien , devient le paragraphe  .\n",
      "Dans les exemples de marques d’homologation et dans les légendes situées en dessous , remplacer le numéro d’homologation “ par “ .\n",
      "La légende en dessous du modèle B , modifier comme suit  : \n",
      "OBSERVATIONS ECRITES DES ETATS MEMBRES \n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 20)\n",
    "\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word.lower(): None for word in source_text.split()})))\n",
    "\n",
    "\n",
    "print(\"-\"*5 + \"English Text\" + \"-\"*5)\n",
    "sentences = source_text.split('\\n')\n",
    "word_counts = [len(sentence.split()) for sentence in sentences]\n",
    "print('Number of sentences: {}'.format(len(sentences)))\n",
    "print('Average number of words in a sentence: {}'.format(np.average(word_counts)))\n",
    "print('Max number of words in a sentence: {}'.format(np.max(word_counts)))\n",
    "\n",
    "print()\n",
    "print(\"-\"*5 + \"French Text\" + \"-\"*5)\n",
    "sentences = target_text.split('\\n')\n",
    "word_counts = [len(sentence.split()) for sentence in sentences]\n",
    "print('Number of sentences: {}'.format(len(sentences)))\n",
    "print('Average number of words in a sentence: {}'.format(np.average(word_counts)))\n",
    "print('Max number of words in a sentence: {}'.format(np.max(word_counts)))\n",
    "\n",
    "print()\n",
    "print('English sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(source_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))\n",
    "print()\n",
    "print('French sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(target_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haiti\n",
      "740\n",
      "0.795\n",
      "0.8\n",
      "0.805\n",
      "0.81\n",
      "0.8150000000000001\n",
      "0.8200000000000001\n",
      "0.8250000000000001\n",
      "0.8300000000000001\n",
      "0.8350000000000001\n",
      "0.8400000000000001\n",
      "0.8450000000000001\n",
      "0.8500000000000001\n",
      "0.8550000000000001\n",
      "0.8600000000000001\n",
      "0.8650000000000001\n",
      "0.8700000000000001\n",
      "0.8750000000000001\n",
      "0.8800000000000001\n",
      "0.8850000000000001\n",
      "0.8900000000000001\n",
      "0.8950000000000001\n",
      "0.9000000000000001\n",
      "0.9050000000000001\n",
      "0.9100000000000001\n",
      "0.9150000000000001\n",
      "0.9200000000000002\n",
      "0.9250000000000002\n",
      "0.9300000000000002\n",
      "0.9350000000000002\n",
      "0.9400000000000002\n",
      "vocabRatio 0.9400000000000002\n",
      "s'être\n",
      "75\n",
      "The size of English vocab is : 4010\n",
      "The size of French vocab is : 5298\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "vocabRatio=0.79\n",
    "from collections import Counter \n",
    "\n",
    "source_text_forvaca = [item for items, c in Counter(source_text.lower().split()).most_common() \n",
    "                                      for item in [items] * c] \n",
    "source_text_forvacac = [c for items, c in Counter(source_text.lower().split()).most_common() \n",
    "                                      for item in [items] * c] \n",
    "print(source_text_forvaca[int(len(source_text_forvaca)*vocabRatio)])\n",
    "print(source_text_forvacac[int(len(source_text_forvaca)*vocabRatio)])\n",
    "\n",
    "\n",
    "source_vocab=[]\n",
    "while(len(source_vocab)<uniquewordscnt and vocabRatio<1.0):\n",
    "    vocabRatio+=0.005\n",
    "    print(vocabRatio)\n",
    "    source_text_forvacaloop=source_text_forvaca[:int(len(source_text_forvaca)*vocabRatio)]\n",
    "    source_vocab = list(set(source_text_forvacaloop))\n",
    "\n",
    "print('vocabRatio',vocabRatio)\n",
    "# 构造法语词典\n",
    "if(vocabRatio<1):\n",
    "    vocabRatio=vocabRatio-0.01\n",
    "target_vocab=[]\n",
    "target_text_forvaca = [item for items, c in Counter(target_text.lower().split()).most_common() \n",
    "                                      for item in [items] * c] \n",
    "target_text_forvacac = [c for items, c in Counter(target_text.lower().split()).most_common() \n",
    "                                      for item in [items] * c] \n",
    "print(target_text_forvaca[int(len(target_text_forvaca)*vocabRatio)-1])\n",
    "print(target_text_forvacac[int(len(target_text_forvaca)*vocabRatio)-1])\n",
    "target_text_forvaca=target_text_forvaca[:int(len(target_text_forvaca)*vocabRatio)]\n",
    "target_vocab = list(set(target_text_forvaca))\n",
    "\n",
    "\n",
    "\n",
    "source_vocab=source_vocab[:20000]\n",
    "target_vocab=target_vocab[:30000]\n",
    "print(\"The size of English vocab is : {}\".format(len(source_vocab)))\n",
    "print(\"The size of French vocab is : {}\".format(len(target_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "23000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "28000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "36000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "50000\n",
      "50000\n",
      "51000\n",
      "51000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "62000\n",
      "63000\n",
      "63000\n",
      "63000\n",
      "63000\n",
      "64000\n",
      "64000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "66000\n",
      "66000\n",
      "67000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "70000\n",
      "70000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "80000\n",
      "80000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "110000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "118000\n",
      "118000\n",
      "119000\n",
      "119000\n",
      "120000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "124000\n",
      "124000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "146000\n",
      "146000\n",
      "146000\n",
      "147000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n",
      "166000\n",
      "167000\n",
      "168000\n",
      "169000\n",
      "170000\n",
      "171000\n",
      "171000\n",
      "171000\n",
      "172000\n",
      "173000\n",
      "174000\n",
      "175000\n",
      "176000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "179000\n",
      "180000\n",
      "181000\n",
      "182000\n",
      "182000\n",
      "183000\n",
      "184000\n",
      "185000\n",
      "185000\n",
      "185000\n",
      "185000\n",
      "186000\n",
      "187000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "191000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "199000\n",
      "200000\n",
      "201000\n",
      "202000\n",
      "203000\n",
      "204000\n",
      "205000\n",
      "205000\n",
      "206000\n",
      "207000\n",
      "208000\n",
      "209000\n",
      "210000\n",
      "211000\n",
      "212000\n",
      "213000\n",
      "213000\n",
      "213000\n",
      "214000\n",
      "214000\n",
      "215000\n",
      "215000\n",
      "216000\n",
      "216000\n",
      "217000\n",
      "218000\n",
      "219000\n",
      "220000\n",
      "221000\n",
      "222000\n",
      "223000\n",
      "224000\n",
      "225000\n",
      "226000\n",
      "227000\n",
      "228000\n",
      "228000\n",
      "229000\n",
      "230000\n",
      "231000\n",
      "232000\n",
      "233000\n",
      "234000\n",
      "235000\n",
      "236000\n",
      "237000\n",
      "238000\n",
      "238000\n",
      "239000\n",
      "240000\n",
      "240000\n",
      "241000\n",
      "242000\n",
      "242000\n",
      "243000\n",
      "243000\n",
      "244000\n",
      "245000\n",
      "245000\n",
      "245000\n",
      "246000\n",
      "247000\n",
      "248000\n",
      "249000\n",
      "250000\n",
      "251000\n",
      "252000\n",
      "253000\n",
      "253000\n",
      "254000\n",
      "254000\n",
      "255000\n",
      "256000\n",
      "257000\n",
      "258000\n",
      "259000\n",
      "260000\n",
      "260000\n",
      "261000\n",
      "262000\n",
      "263000\n",
      "264000\n",
      "265000\n",
      "266000\n",
      "267000\n",
      "267000\n",
      "268000\n",
      "269000\n",
      "270000\n",
      "271000\n",
      "272000\n",
      "273000\n",
      "274000\n",
      "275000\n",
      "276000\n",
      "277000\n",
      "278000\n",
      "278000\n",
      "279000\n",
      "280000\n",
      "280000\n",
      "281000\n",
      "281075\n",
      "0\n",
      "1000\n",
      "2000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "14000\n",
      "15000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "27000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "30000\n",
      "30000\n",
      "30000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "33000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "48000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "51000\n",
      "52000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "84000\n",
      "85000\n",
      "85000\n",
      "86000\n",
      "86000\n",
      "86000\n",
      "86000\n",
      "86000\n",
      "87000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "91000\n",
      "92000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "107000\n",
      "107000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "110000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n",
      "166000\n",
      "166000\n",
      "167000\n",
      "168000\n",
      "168000\n",
      "168000\n",
      "169000\n",
      "170000\n",
      "171000\n",
      "172000\n",
      "173000\n",
      "174000\n",
      "175000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "179000\n",
      "180000\n",
      "180000\n",
      "180000\n",
      "181000\n",
      "182000\n",
      "183000\n",
      "184000\n",
      "185000\n",
      "186000\n",
      "187000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "190000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "199000\n",
      "200000\n",
      "200000\n",
      "201000\n",
      "202000\n",
      "203000\n",
      "203000\n",
      "203000\n",
      "204000\n",
      "204000\n",
      "205000\n",
      "206000\n",
      "207000\n",
      "208000\n",
      "209000\n",
      "209000\n",
      "210000\n",
      "211000\n",
      "211000\n",
      "212000\n",
      "213000\n",
      "214000\n",
      "215000\n",
      "216000\n",
      "217000\n",
      "218000\n",
      "219000\n",
      "220000\n",
      "221000\n",
      "222000\n",
      "222000\n",
      "223000\n",
      "224000\n",
      "225000\n",
      "226000\n",
      "227000\n",
      "228000\n",
      "229000\n",
      "230000\n",
      "231000\n",
      "232000\n",
      "233000\n",
      "233704\n",
      "233704\n",
      "233704\n"
     ]
    }
   ],
   "source": [
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "source_textlist=source_text.splitlines(keepends=True)\n",
    "target_textlist=target_text.splitlines(keepends=True)\n",
    "\n",
    "i=-1\n",
    "\n",
    "while(i<len(source_textlist)):\n",
    "    i+=1\n",
    " \n",
    "    if(i%1000==0):\n",
    "        print(i)\n",
    "    try:\n",
    "        line=source_textlist[i]\n",
    "    except:\n",
    "        print(i)\n",
    "        break  \n",
    "    wordcnt=0\n",
    "    unkcnt=0\n",
    "   \n",
    "    line=line.split()\n",
    "    for word in line:\n",
    "        wordcnt+=1\n",
    "        if(word.lower() not in source_vocab ):\n",
    "       \n",
    "            unkcnt+=1\n",
    "    \n",
    "    \n",
    "    if(unkcnt>unkcntlimit or unkcnt/wordcnt>0.2):\n",
    "      \n",
    "      \n",
    "        del source_textlist[i]\n",
    "        del target_textlist[i]\n",
    "        i-=1\n",
    "\n",
    "\n",
    "i=-1\n",
    "while(i<len(target_textlist)):\n",
    "    i+=1\n",
    "    if(i%1000==0):\n",
    "        print(i)\n",
    "    try:\n",
    "        line=target_textlist[i]\n",
    "    except:\n",
    "        print(i)\n",
    "        break  \n",
    "    wordcnt=0\n",
    "    unkcnt=0\n",
    "   \n",
    "    line=line.split()\n",
    "    for word in line:\n",
    "        wordcnt+=1\n",
    "        if(word.lower() not in target_vocab ):\n",
    "\n",
    "            unkcnt+=1\n",
    "            \n",
    "    \n",
    "    \n",
    "    if(unkcnt>unkcntlimit or unkcnt/wordcnt>0.2):\n",
    "\n",
    "        del source_textlist[i]\n",
    "        del target_textlist[i]\n",
    "        i-=1\n",
    "\n",
    "\n",
    "print(len(source_textlist))\n",
    "print(len(target_textlist))\n",
    "\n",
    "\n",
    "source_text=''.join(source_textlist)\n",
    "target_text=''.join(target_textlist)\n",
    "del source_textlist\n",
    "del target_textlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of English vocab is : 4010\n",
      "The size of French vocab is : 5298\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "source_vocab=source_vocab[:20000]\n",
    "target_vocab=target_vocab[:30000]\n",
    "print(\"The size of English vocab is : {}\".format(len(source_vocab)))\n",
    "print(\"The size of French vocab is : {}\".format(len(target_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of English vocab is : 4010\n",
      "The size of French vocab is : 5298\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of English vocab is : {}\".format(len(source_vocab)))\n",
    "print(\"The size of French vocab is : {}\".format(len(target_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特殊字符\n",
    "SOURCE_CODES = ['<PAD>', '<UNK>']\n",
    "TARGET_CODES = ['<PAD>', '<EOS>', '<UNK>', '<GO>']  # 在target中，需要增加<GO>与<EOS>特殊字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造英文映射字典\n",
    "source_vocab_to_int = {word: idx for idx, word in enumerate(SOURCE_CODES + source_vocab)}\n",
    "\n",
    "source_int_to_vocab = {idx: word for idx, word in enumerate(SOURCE_CODES + source_vocab)}\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# 构造法语映射词典\n",
    "target_vocab_to_int = {word: idx for idx, word in enumerate(TARGET_CODES + target_vocab)}\n",
    "target_int_to_vocab = {idx: word for idx, word in enumerate(TARGET_CODES + target_vocab)}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "source_vocab_to_int={}\n",
    "for idx, word in enumerate(SOURCE_CODES + source_vocab):\n",
    "    source_vocab_to_int[word]=idx\n",
    "    \n",
    "source_int_to_vocab={}\n",
    "for idx, word in enumerate(SOURCE_CODES + source_vocab):\n",
    "    source_int_to_vocab[idx]=word\n",
    "    \n",
    "target_vocab_to_int={}\n",
    "for idx, word in enumerate(TARGET_CODES + target_vocab):\n",
    "    target_vocab_to_int[word]=idx\n",
    "    \n",
    "target_int_to_vocab={}\n",
    "for idx, word in enumerate(TARGET_CODES + target_vocab):\n",
    "    target_int_to_vocab[idx]=word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of English Map is : 4012\n",
      "The size of French Map is : 5302\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of English Map is : {}\".format(len(source_vocab_to_int)))\n",
    "print(\"The size of French Map is : {}\".format(len(target_vocab_to_int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essentiellement\n"
     ]
    }
   ],
   "source": [
    "print(target_int_to_vocab[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checksentence(sentence):\n",
    "    invalid=0\n",
    "    total=1\n",
    "   # for word in sentence.lower().split():\n",
    "    #    if(word.isalpha()==False and word !=',' and word !='.'):\n",
    "    #        invalid+=1\n",
    "    #    total+=1\n",
    "    #if(total<3):\n",
    "    #    return False\n",
    "   # if(invalid/total>0.2):\n",
    "\n",
    "   #     return False\n",
    "    return True\n",
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def text_to_int(sentence, map_dict, max_length=maxlength, is_target=False):\n",
    "\n",
    "    \n",
    "    \n",
    "    text_to_idx = []\n",
    "    # unk index\n",
    "    unk_idx = map_dict.get(\"<UNK>\")\n",
    "    pad_idx = map_dict.get(\"<PAD>\")\n",
    "    eos_idx = map_dict.get(\"<EOS>\")\n",
    "    \n",
    "  \n",
    "    \n",
    "    for word in sentence.lower().split():\n",
    "        if(hasNumbers(word)):\n",
    "            text_to_idx.append(unk_idx)\n",
    "        else:\n",
    "            text_to_idx.append(map_dict.get(word, unk_idx))\n",
    "    if is_target:\n",
    "        text_to_idx.append(eos_idx)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    if len(text_to_idx) > max_length:\n",
    "        return text_to_idx[:max_length]\n",
    "\n",
    "    else:\n",
    "        text_to_idx = text_to_idx + [pad_idx] * (max_length - len(text_to_idx))\n",
    "        return text_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233705/233705 [00:05<00:00, 45315.12it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "source_text_to_int = []\n",
    "target_text_to_int = []\n",
    "sourcelist=source_text.split(\"\\n\")\n",
    "targetlist=target_text.split(\"\\n\")\n",
    "index=0\n",
    "for index in tqdm.tqdm(range(len(sourcelist))):\n",
    "    sentence=sourcelist[index]\n",
    "    if(checksentence(sentence)):\n",
    "        source_text_to_int.append(text_to_int(sentence, source_vocab_to_int, maxlength, \n",
    "                                          is_target=False))\n",
    "        target_text_to_int.append(text_to_int(targetlist[index], target_vocab_to_int, maxlength, \n",
    "                                              is_target=True))\n",
    "    else:\n",
    "        print(sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "target_text_to_int = []\n",
    "\n",
    "for sentence in tqdm.tqdm(target_text.split(\"\\n\")):\n",
    "    target_text_to_int.append(text_to_int(sentence, target_vocab_to_int, maxlength, \n",
    "                                          is_target=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----English example-----\n",
      "A . The overall policy environment     \n",
      "[3567, 3138, 1525, 1187, 76, 1799, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "-----French example-----\n",
      "A . Contexte général des politiques de la dette     \n",
      "[4663, 4110, 432, 1126, 4225, 4056, 3125, 2402, 4582, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "-----English example-----\n",
      "B . A focus on the Paris Club     \n",
      "[458, 3138, 3567, 530, 2557, 1525, 1672, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "-----French example-----\n",
      "B . Le Club de Paris     \n",
      "[572, 4110, 2976, 2, 3125, 2234, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "random_index = 20000\n",
    "\n",
    "print(\"-\"*5 + \"English example\" + \"-\"*5)\n",
    "print(source_text.split(\"\\n\")[random_index])\n",
    "print(source_text_to_int[random_index])\n",
    "\n",
    "print()\n",
    "print(\"-\"*5 + \"French example\" + \"-\"*5)\n",
    "print(target_text.split(\"\\n\")[random_index])\n",
    "print(target_text_to_int[random_index])\n",
    "\n",
    "print(\"-\"*5 + \"English example\" + \"-\"*5)\n",
    "print(source_text.split(\"\\n\")[random_index+1])\n",
    "print(source_text_to_int[random_index+1])\n",
    "\n",
    "print()\n",
    "print(\"-\"*5 + \"French example\" + \"-\"*5)\n",
    "print(target_text.split(\"\\n\")[random_index+1])\n",
    "print(target_text_to_int[random_index+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(233705, 30)\n",
      "(233705, 30)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(source_text_to_int)\n",
    "Y = np.array(target_text_to_int)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose2(tensor,perm):\n",
    "    x=tf.transpose(tensor, perm=perm)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda, Embedding\n",
    "\n",
    "with open(\"../data/enwiki_20180420_100d.txt\", 'r') as f:\n",
    "    words = set()\n",
    "    word_to_vec_map = {}\n",
    "    iii=0\n",
    "    for line in f:\n",
    "        iii+=1\n",
    "      \n",
    "        if(iii==1):\n",
    "            continue\n",
    "        line = line.strip().split()\n",
    "        curr_word = line[0]\n",
    "        if(curr_word[:5]=='ENTITY'):\n",
    "            continue\n",
    "\n",
    "        try:       \n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            words.add(curr_word)\n",
    "        except:\n",
    "            continue       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "        \n",
    "        \n",
    "def pretrained_embedding_layer(word_to_vec_map, source_vocab_to_int):\n",
    "\n",
    "    \n",
    "    vocab_len = len(source_vocab_to_int)      \n",
    "    emb_dim = word_to_vec_map[\"the\"].shape[0]\n",
    "\n",
    "\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    " \n",
    "    for word, index in source_vocab_to_int.items():\n",
    "        word_vector = word_to_vec_map.get(word, np.zeros(emb_dim))\n",
    "        emb_matrix[index, :] = word_vector\n",
    "\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "\n",
    "    # build\n",
    "    embedding_layer.build((None,))\n",
    "\n",
    "    # set weights\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer\n",
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, source_vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0861 -0.1222 -0.0619 -0.3368  0.115  -0.2224  0.4958  0.1807 -0.1449\n",
      "  0.1074  0.1161 -0.0583 -0.0113 -0.4103 -0.0344  0.4085  0.1163  0.3438\n",
      " -0.0576 -0.0821 -0.1362 -0.1616  0.1844  0.182   0.1505 -0.1365  0.2082\n",
      "  0.2805 -0.1366  0.1316  0.0174 -0.0438  0.2987  0.03    0.3864  0.1577\n",
      "  0.0414 -0.1326  0.0391  0.2384  0.2411 -0.0183 -0.0548 -0.2325 -0.2505\n",
      "  0.0733  0.1344 -0.0767 -0.1943 -0.2422  0.1456  0.1247 -0.3724 -0.3578\n",
      "  0.0514 -0.1402  0.102  -0.2958 -0.0759 -0.0165 -0.1523  0.1517  0.2418\n",
      " -0.3599  0.6086 -0.3629 -0.0387 -0.0588  0.1927  0.2059 -0.0409  0.2896\n",
      " -0.0852  0.1295  0.2394 -0.0134 -0.2348 -0.0374  0.0492 -0.3214  0.1219\n",
      " -0.0497 -0.252  -0.1881 -0.424  -0.5537 -0.1215 -0.2991  0.2913  0.0731\n",
      " -0.2589 -0.0038  0.4187 -0.1844 -0.2661 -0.1089  0.2849 -0.1919  0.1371\n",
      " -0.2906]\n",
      "2500667\n",
      "<class 'dict'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda, Embedding\n",
    "\n",
    "with open(\"../data/frwiki_20180420_100d.txt\", 'r') as fr:\n",
    "#with open(\"../data/glove.42B.300d.txt\", 'r') as f:\n",
    "    words = set()\n",
    "    word_to_vec_mapfr = {}\n",
    "    iii=0\n",
    "    for line in fr:\n",
    "        iii+=1\n",
    "        if(iii==1):\n",
    "            continue\n",
    "        line = line.strip().split()\n",
    "        curr_word = line[0]\n",
    "        if(curr_word[:5]=='ENTITY'):\n",
    "            continue\n",
    "        #print(line[1])\n",
    "\n",
    "\n",
    "        try:\n",
    "\n",
    "\n",
    "       \n",
    "            word_to_vec_mapfr[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            words.add(curr_word)\n",
    "        except:\n",
    "            continue\n",
    "print(word_to_vec_mapfr['de'])\n",
    "print(len(word_to_vec_mapfr))\n",
    "print(type(target_vocab_to_int))\n",
    "print(type(target_text_to_int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(pretrain_fr==1):\n",
    "    def pretrained_embedding_layerfr(word_to_vec_mapfr, target_vocab_to_int):\n",
    "        \"\"\"\n",
    "        构造Embedding层并加载预训练好的词向量（这里我使用的是100维）\n",
    "\n",
    "        @param word_to_vec_map: 单词到向量的映射\n",
    "        @param word_to_index: 单词到数字编码的映射\n",
    "        \"\"\"\n",
    "\n",
    "        vocab_len = len(target_vocab_to_int)    \n",
    "        emb_dim = word_to_vec_mapfr[\"de\"].shape[0]\n",
    "\n",
    "      \n",
    "        emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "\n",
    "\n",
    "        for word, index in target_vocab_to_int.items():\n",
    "            word_vector = word_to_vec_mapfr.get(word, np.zeros(emb_dim))\n",
    "            emb_matrix[index, :] = word_vector\n",
    "\n",
    "\n",
    "        embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "\n",
    "        # build\n",
    "        embedding_layer.build((None,))\n",
    "\n",
    "        # set weights\n",
    "        embedding_layer.set_weights([emb_matrix])\n",
    "\n",
    "        return embedding_layer\n",
    "    embedding_layerfr = pretrained_embedding_layerfr(word_to_vec_mapfr, target_vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(pretrain_fr==0):\n",
    "    class EmbeddingFrTrain(tf.keras.layers.Layer):\n",
    "        def __init__(self, vocabcnt, wordvectorlength):\n",
    "            super(EmbeddingFrTrain, self).__init__()\n",
    "\n",
    "            self.vocabcnt=vocabcnt\n",
    "            self.wordvectorlength=wordvectorlength\n",
    "            #self.lstm_cell = LSTM(rnn_size,return_state=True,dropout=decoderlstmdropout,recurrent_dropout=decoderrecurdropout,return_sequences=False,name='decoder_cell')\n",
    "\n",
    "\n",
    "        def get_config(self):\n",
    "\n",
    "            config = super().get_config().copy()\n",
    "            config.update({\n",
    "\n",
    "                'vocabcnt':self.vocabcnt,\n",
    "                'wordvectorlength':self.wordvectorlength,\n",
    "\n",
    "            })\n",
    "            return config\n",
    "\n",
    "        def call(self, inputx):\n",
    "\n",
    "            out=Embedding(self.vocabcnt, self.wordvectorlength)(inputx)\n",
    "            #print(outputa,cin,encoder_outputsin,outputd_embed)\n",
    "\n",
    "            #outputd=Lambda(lambda x:K.expand_dims(x,1))(outputd) #1, rnnsize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            return out\n",
    "\n",
    "\n",
    "    embeddingFrTrain=EmbeddingFrTrain(len(target_vocab_to_int)  ,wordvectorsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.keras.layers.embeddings.Embedding'>\n"
     ]
    }
   ],
   "source": [
    "print(type(embedding_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(rnn_inputs, rnn_size=200, rnn_num_layers=1,\n",
    "                  source_sequence_len=maxlength):\n",
    "\n",
    "\n",
    "    x=embedding_layer(rnn_inputs)\n",
    "\n",
    " \n",
    "    \n",
    "    rnn_sizenotfirst=int(rnn_size)\n",
    " \n",
    "    for i in range(layerofencoderlstm):\n",
    "        x, state_h, state_c,backward_h,backward_c=Bidirectional(LSTM(int(rnn_sizenotfirst),return_sequences=True,dropout=lstmdropout,return_state=True,recurrent_dropout=recur_dropout, kernel_regularizer=lstmregulizer, \n",
    "                                  recurrent_regularizer=lstmregulizer))(x)\n",
    "    state_c = Concatenate()([state_c, backward_c])\n",
    "    print('x',x)\n",
    "    #x, state_h, state_c =LSTM(rnn_size,return_state=True,return_sequences=True,unroll=True) (x)\n",
    "   \n",
    "\n",
    "    #state_c=Dropout(betweenlayerdropout)(state_c)\n",
    "    #state_c=Dense(rnn_size)(state_c)\n",
    "    encoder_outputs=x\n",
    "    encoder_states=state_c\n",
    "    #print('encoder_states',encoder_states)\n",
    "    #encoder_states= K.zeros_like(encoder_states)\n",
    " \n",
    "    return encoder_outputs, encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer_inputs(target_data, target_vocab_to_int):\n",
    "\n",
    "    withoutlast=target_data[:,:-1]\n",
    "   # withoutlast = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "\n",
    "    decoder_inputs = tf.concat([tf.fill([tf.shape(withoutlast)[0], 1], tf.cast(target_vocab_to_int[\"<GO>\"],tf.float32)), \n",
    "                                withoutlast], 1)\n",
    "    \n",
    "    return decoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(attention=='Loung'):\n",
    "    class Loung(tf.keras.layers.Layer):  #attention class\n",
    "        def __init__(self, rnn_size,maxlength):\n",
    "            super(Loung, self).__init__()\n",
    "            self.W1 = Dense(wordvectorsize,activation='tanh')\n",
    "            self.W2 = Dense(1,activation='relu')\n",
    "            self.W3 = Dense(loungAttentionNodeCnt,activation=activationF,kernel_regularizer=denseregulizer,\n",
    "                    activity_regularizer=denseregulizer)\n",
    "            self.rnn_size=rnn_size\n",
    "            self.maxlength=maxlength\n",
    "            #self.lstm_cell = LSTM(rnn_size,return_state=True,dropout=decoderlstmdropout,recurrent_dropout=decoderrecurdropout,return_sequences=False,name='decoder_cell')\n",
    "            self.lstm_cell = LSTM(rnn_size*2,return_state=True,\n",
    "                                  dropout=decoderlstmdropout,\n",
    "                                  recurrent_dropout=decoderrecurdropout,\n",
    "                                return_sequences=False,\n",
    "                                  kernel_regularizer=lstmregulizer, \n",
    "                                  recurrent_regularizer=lstmregulizer\n",
    "\n",
    "                                 )\n",
    "\n",
    "        def get_config(self):\n",
    "\n",
    "            config = super().get_config().copy()\n",
    "            config.update({\n",
    "\n",
    "                'rnn_size':self.rnn_size,\n",
    "                'maxlength':self.maxlength,\n",
    "\n",
    "            })\n",
    "            return config\n",
    "\n",
    "        def call(self, outputa,cin,encoder_outputsin,outputd_embed,outputdlast):\n",
    "\n",
    "            #print(outputa,cin,encoder_outputsin,outputd_embed)\n",
    "\n",
    "            #outputd=Lambda(lambda x:K.expand_dims(x,1))(outputd) #1, rnnsize\n",
    "\n",
    "\n",
    "            ###!!!!!!!!!!!!!!!!!!!!!!!!!!!#########\n",
    "            zeros = Lambda(lambda x: tf.zeros_like(x))(cin)\n",
    "            #print('outputa',outputa)\n",
    "\n",
    "            outputa=tf.reshape(outputa,(-1,wordvectorsize))\n",
    "            outputdlast=tf.reshape(outputdlast,(-1,loungAttentionNodeCnt))\n",
    "            #print('Concatenate1inpuy',outputa,outputdlast)\n",
    "            outputa=Concatenate(axis=-1)([outputa,outputdlast])\n",
    "            outputa=tf.expand_dims(outputa, 1)\n",
    "\n",
    "\n",
    "            #print('input',outputa,cin,encoder_outputsin,outputd_embed)\n",
    "            outputa,_,  c = self.lstm_cell(outputa,initial_state= [zeros,cin] )  \n",
    "           # print('lstmout',outputa,c)\n",
    "\n",
    "            outputd=RepeatVector(maxlength)(outputa) #25, wordvectorsize\n",
    "          #  print('RepeatVectorout',outputd)\n",
    "            concate=Concatenate(axis=-1)([encoder_outputsin,outputd]) #25 2*rnnsize+2*rnnsize\n",
    "           # print('Concatenateout',concate)\n",
    "            concate=self.W1(concate) #25,100 \n",
    "            #print('concate',concate)\n",
    "\n",
    "            score=self.W2(concate) #25,1 ############score\n",
    "            #score=tf.matmul(s2,outputd,transpose_a=True) #100,25     25,100\n",
    "            #print('score',score)\n",
    "            #100*100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            attention_weights = Softmax(axis=-2)(score) #25,1\n",
    "            #print('attention_weights',attention_weights)\n",
    "            attention_weights=K.squeeze(attention_weights,axis=-1)\n",
    "        \n",
    "\n",
    "            context_vector = Dot(axes=1)([attention_weights,encoder_outputsin]) #25,1   25 rnnsize ->#1,rnnsize\n",
    "\n",
    "\n",
    "            #print('Concatenateout2',concate)\n",
    "            outputd = tf.concat([tf.expand_dims(context_vector, 1), tf.expand_dims(outputa, 1)], axis=-1)  #1,rnnsize+(wordvectorsize)\n",
    "            #print('outputdAfterconcat',outputd)\n",
    "            #outputd = tf.concat([tf.expand_dims(context_vector, 1),outputsoft], axis=-1) \n",
    "\n",
    "\n",
    "            #print('outputd',outputd)\n",
    "            outputd=self.W3(outputd)\n",
    "            outputd=Dropout(betweenlayerdropout)(outputd)\n",
    "\n",
    "            return outputd, c\n",
    "\n",
    "\n",
    "    attention_decoder1=Loung(rnn_size,maxlength)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if(attention=='Bahdanau'):   \n",
    "    class Bahdanau(tf.keras.layers.Layer):\n",
    "        def __init__(self, rnn_size,maxlength):\n",
    "            super(Bahdanau, self).__init__()\n",
    "            self.W1 = Dense(wordvectorsize,activation='tanh')\n",
    "            self.W2 = Dense(1,activation='relu')\n",
    "            self.W3 = Dense(wordvectorsize,activation=activationF,kernel_regularizer=denseregulizer,\n",
    "                    activity_regularizer=denseregulizer)\n",
    "            self.rnn_size=rnn_size\n",
    "            self.maxlength=maxlength\n",
    "            #self.lstm_cell = LSTM(rnn_size,return_state=True,dropout=decoderlstmdropout,recurrent_dropout=decoderrecurdropout,return_sequences=False,name='decoder_cell')\n",
    "            self.lstm_cell = LSTM(rnn_size*2,return_state=True,\n",
    "                                  dropout=decoderlstmdropout,\n",
    "                                  recurrent_dropout=decoderrecurdropout,\n",
    "                                return_sequences=False,\n",
    "                                  kernel_regularizer=lstmregulizer, \n",
    "                                  recurrent_regularizer=lstmregulizer\n",
    "\n",
    "                                 )\n",
    "\n",
    "        def get_config(self):\n",
    "\n",
    "            config = super().get_config().copy()\n",
    "            config.update({\n",
    "\n",
    "                'rnn_size':self.rnn_size,\n",
    "                'maxlength':self.maxlength,\n",
    "\n",
    "            })\n",
    "            return config\n",
    "\n",
    "        def call(self, outputa,cin,encoder_outputsin,outputd_embed,outputdlast):\n",
    "\n",
    "            #print(outputa,cin,encoder_outputsin,outputd_embed)\n",
    "\n",
    "            #outputd=Lambda(lambda x:K.expand_dims(x,1))(outputd) #1, rnnsize\n",
    "\n",
    "            #print(outputd)\n",
    "            ###!!!!!!!!!!!!!!!!!!!!!!!!!!!#########\n",
    "            outputd=RepeatVector(maxlength)(outputa) #25, wordvectorsize\n",
    "            concate=Concatenate(axis=-1)([encoder_outputsin,outputd]) #25 rnnsize+wordvectorsize\n",
    "            concate=self.W1(concate) #25,100 \n",
    "            #print('concate',concate)\n",
    "\n",
    "            score=self.W2(concate) #25,1 ############score\n",
    "            #score=tf.matmul(s2,outputd,transpose_a=True) #100,25     25,100\n",
    "            #print('score',score)\n",
    "            #100*100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            attention_weights = Softmax(axis=-2)(score) #25,1\n",
    "            #print('attention_weights',attention_weights)\n",
    "            attention_weights=K.squeeze(attention_weights,axis=-1)\n",
    "            #attention_weights=tf.transpose(attention_weights,perm=[0,2,1])\n",
    "            #print('attention_weights',attention_weights)  \n",
    "            #print('encoder_outputs',encoder_outputs)\n",
    "\n",
    "            context_vector = Dot(axes=1)([attention_weights,encoder_outputsin]) #25,1   25 rnnsize ->#1,rnnsize, \n",
    "            #print('context_vector',context_vector)\n",
    "            #context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "            #print('context_vector',context_vector)\n",
    "            ###########!!!!!!!!!!!!!!!!!!!!###################\n",
    "            outputd = tf.concat([tf.expand_dims(context_vector, 1), tf.expand_dims(outputd_embed, 1)], axis=-1)  #1,rnnsize+(wordvectorsize)\n",
    "            #print('outputdAfterconcat',outputd)\n",
    "            #outputd = tf.concat([tf.expand_dims(context_vector, 1),outputsoft], axis=-1) \n",
    "            zeros = Lambda(lambda x: tf.zeros_like(x))(cin)\n",
    "            print('lstmin',outputd,cin)\n",
    "            outputd,_,  c = self.lstm_cell(outputd,initial_state= [zeros,cin] )\n",
    "            #print('outputd',outputd)\n",
    "            #outputd=self.W3(outputd)\n",
    "            outputd=Dropout(betweenlayerdropout)(outputd)\n",
    "\n",
    "            return outputd, c\n",
    "\n",
    "\n",
    "    attention_decoder1=Bahdanau(rnn_size,maxlength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 - Decoder Traing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer_the(encoder_outputs,encoder_states, is_training, decoder_embed,\n",
    "                        target_sequence_len, max_target_sequence_len,rnn_size):\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    outputsall=[]\n",
    "\n",
    "    c=encoder_states\n",
    "    outputd_embed=tf.zeros((tf.shape(encoder_states)[0],wordvectorsize))\n",
    "    if(attention=='Loung'):\n",
    "        \n",
    "        outputd=tf.zeros((tf.shape(encoder_states)[0],loungAttentionNodeCnt))\n",
    "    else:\n",
    "        outputd=tf.zeros((tf.shape(encoder_states)[0],wordvectorsize))\n",
    "   # print('c',c)\n",
    "\n",
    "    #print(decoder_embed) #vectors of 100\n",
    "    #attention_decoder=attention_decoder(rnn_size,target_sequence_len)\n",
    "    \n",
    "\n",
    "    for i in range(max_target_sequence_len):\n",
    "        if(forceteach==1):\n",
    "            if(is_training==1):\n",
    "                \n",
    "                random=K.random_uniform(\n",
    "                    (1,),\n",
    "                    minval=0.0,\n",
    "                    maxval=5.0,\n",
    "\n",
    "                )\n",
    "                random=int(K.get_value(random))\n",
    "                a=i+2-random #(i+1 - (random 0 to 2))\n",
    "                if (a<0):\n",
    "                    a=0\n",
    "                outputd_embed=decoder_embed[:,a]  #go\n",
    "\n",
    "         \n",
    "\n",
    "            \n",
    "            else:\n",
    "                if(i==0):\n",
    "                    outputd_embed=decoder_embed[:,i]  #go\n",
    "\n",
    "        \n",
    "                else:\n",
    "                    outputd_embed=outputd_embed\n",
    "        else: #forceteach==0\n",
    "            outputd_embed=outputd_embed\n",
    "\n",
    "            #outputd= Lambda(lambda x: K.in_train_phase(decoder_embed[:,i],outputd))(output)\n",
    "        \n",
    "        outputd,c=attention_decoder1(outputd_embed,c,encoder_outputs,outputd_embed,outputd)\n",
    "     \n",
    "        \n",
    "        outputdsoftmax=Dense(len(target_vocab_to_int),activation='softmax')(outputd)\n",
    "        outputd_embed=K.argmax(outputdsoftmax)\n",
    "        print('outputdbeforeembedding',outputd_embed)\n",
    "        if(pretrain_fr==1):\n",
    "            \n",
    "            outputd_embed=embedding_layerfr(outputd_embed)\n",
    "        else:\n",
    "            outputd_embed=embeddingFrTrain(outputd_embed)\n",
    "      \n",
    "        print(outputd_embed)\n",
    "       \n",
    "        outputsall.append(outputdsoftmax)\n",
    "\n",
    "             \n",
    "\n",
    "    #print('K.is_keras_tensor(outputsall)',K.is_keras_tensor(outputsall[0]))\n",
    "    print('outputd',outputd)\n",
    "    print('outputsall',outputsall)\n",
    "    outputsall=Lambda(lambda x:K.stack(x,axis=1))(outputsall)\n",
    "    print('outputsall',outputsall)\n",
    "    #outputsall = Lambda(lambda x: K.squeeze(x,2))(outputsall)\n",
    "   # outputsall=outputsall+outputd\n",
    "    print('outputsall2',outputsall)\n",
    "    return outputsall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(encoder_outputs,encoder_states,is_training, decoder_inputs, \n",
    "                    rnn_size, rnn_num_layers,\n",
    "                   target_vocab_to_int,\n",
    "                  target_sequence_len=maxlength,\n",
    "                  max_target_sequence_len=maxlength):\n",
    "\n",
    "    \n",
    "    if(pretrain_fr==1):\n",
    "        \n",
    "        decoder_embed=embedding_layerfr(decoder_inputs) #output as vector\n",
    "    else:\n",
    "        \n",
    "        decoder_embed=embeddingFrTrain(decoder_inputs)\n",
    "    \n",
    "\n",
    "    \n",
    "    # output_layer logits\n",
    "\n",
    "    \n",
    "    \n",
    "    logits = decoder_layer_the(encoder_outputs,encoder_states,is_training,\n",
    "\n",
    "                                           decoder_embed,\n",
    "                                           target_sequence_len,\n",
    "                                           max_target_sequence_len,\n",
    "                                         rnn_size=rnn_size)\n",
    "    \n",
    "\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(  \n",
    "                 shape,source_sequence_len, target_sequence_len, max_target_sequence_len,\n",
    "          \n",
    "          \n",
    "                 rnn_size, rnn_num_layers, target_vocab_to_int):\n",
    "    \n",
    "    input_data = Input(shape=shape)\n",
    "    target_data= Input(shape=shape)\n",
    "  \n",
    "\n",
    "\n",
    "    encoder_outputs, encoder_states = encoder_layer(input_data, rnn_size, rnn_num_layers, source_sequence_len)\n",
    "    \n",
    "\n",
    "    decoder_inputs = decoder_layer_inputs(target_data, target_vocab_to_int)  \n",
    "    \n",
    "    is_training= K.in_train_phase(1,0)\n",
    "    \n",
    "    output = decoder_layer(encoder_outputs,encoder_states,is_training,\n",
    "                                                                       decoder_inputs,\n",
    "                                                                      target_sequence_len=target_sequence_len,\n",
    "                                                                       max_target_sequence_len=max_target_sequence_len,\n",
    "                                                                      rnn_size=rnn_size,\n",
    "                                                                      rnn_num_layers=rnn_num_layers,\n",
    "                                                                      target_vocab_to_int=target_vocab_to_int\n",
    "                                                                     \n",
    "                                                                   \n",
    "                                                                      )\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    model = Model([input_data,target_data], output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "\n",
    "# Batch Size\n",
    "\n",
    "# RNN Size\n",
    "rnn_size = rnn_size\n",
    "# Number of Layers\n",
    "rnn_num_layers = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "x Tensor(\"bidirectional/concat:0\", shape=(?, 30, 800), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_1/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_1:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_2/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_2:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_3/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_3:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_4/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_4:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_5/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_5:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_6/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_6:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_7/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_7:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_8/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_8:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_9/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_9:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_10/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_10:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_11/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_11:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_12/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_12:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_13/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_13:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_14/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_14:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_15/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_15:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_16/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_16:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_17/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_17:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_18/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_18:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_19/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_19:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_20/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_20:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_21/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_21:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_22/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_22:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_23/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_23:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_24/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_24:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_25/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_25:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_26/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_26:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_27/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_27:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_28/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_28:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_29/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputdbeforeembedding Tensor(\"ArgMax_29:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"embedding_1_30/embedding_lookup/Identity_1:0\", shape=(?, 1, 100), dtype=float32)\n",
      "outputd Tensor(\"loung_29/dropout_29/cond/Merge:0\", shape=(?, 1, 400), dtype=float32)\n",
      "outputsall [<tf.Tensor 'dense_3/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_4/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_5/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_6/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_7/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_8/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_9/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_10/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_11/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_12/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_13/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_14/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_15/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_16/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_17/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_18/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_19/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_20/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_21/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_22/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_23/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_24/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_25/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_26/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_27/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_28/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_29/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_30/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_31/truediv:0' shape=(?, 1, 5302) dtype=float32>, <tf.Tensor 'dense_32/truediv:0' shape=(?, 1, 5302) dtype=float32>]\n",
      "outputsall Tensor(\"lambda_30/stack:0\", shape=(?, 30, 1, 5302), dtype=float32)\n",
      "outputsall2 Tensor(\"lambda_30/stack:0\", shape=(?, 30, 1, 5302), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if(loadmodel==0):\n",
    "    model = seq2seq_model(  (maxlength,),\n",
    "                 maxlength, maxlength, maxlength,\n",
    "               \n",
    "                \n",
    "                 rnn_size, rnn_num_layers, target_vocab_to_int)\n",
    "   # model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"./models/bestaccu.hdf5\", \n",
    "                           monitor = 'val_loss',\n",
    "                           mode='min',\n",
    "                           verbose=1, \n",
    "                           save_best_only=True)   \n",
    "tensorboard=TensorBoard(log_dir='./logs')\n",
    "\n",
    "    \n",
    "\n",
    "#model.compile(loss=customLoss, optimizer=adam) \n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=3,verbose=1,min_lr=0.000001)\n",
    "earlystopping=EarlyStopping(monitor='val_loss', patience=50, verbose=2)\n",
    "\n",
    "\n",
    "callbackss=[reduce_lr,earlystopping,\n",
    "           #checkpointer,\n",
    "            tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lossf=tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5302\n"
     ]
    }
   ],
   "source": [
    "print(len(target_vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(source_vocab_to_int.get(\"<UNK>\"))\n",
    "print(target_vocab_to_int.get(\"<UNK>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam=tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.001, amsgrad=True)\n",
    "#adam=tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.001)\n",
    "#adam=tf.keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam)\n",
    "#model.compile(loss=customloss, optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(233705, 30)\n",
      "(233705, 30)\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "#n_values = np.max(Y) + 1\n",
    "#Yhot=np.eye(n_values)[Y]\n",
    "#print(Yhot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2479 3138 2654 3608 1100  349 1525  401 2163 3897    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "i . financial performance report for the period from november <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "[3250 4110 5108  138 1867  190 3354  203 2402 2020    1    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "i . rapport sur l'exécution du budget pour la période <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "indexxx=2000\n",
    "print(X[indexxx])\n",
    "xlist=[]\n",
    "for word in X[indexxx]:\n",
    "    xlist.append(source_int_to_vocab[word])\n",
    "xlist=' '.join(xlist)\n",
    "print(xlist)\n",
    "\n",
    "print(Y[indexxx])\n",
    "ylist=[]\n",
    "for word in Y[indexxx]:\n",
    "    ylist.append(target_int_to_vocab[word])\n",
    "ylist=' '.join(ylist)\n",
    "print(ylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "class_weights =compute_sample_weight('balanced',y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 30, 100)      401200      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   [(None, 30, 800), (N 1603200     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 800)          0           bidirectional[0][2]              \n",
      "                                                                 bidirectional[0][4]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_zeros/packed (Tenso [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_zeros_1/packed (Ten [(2,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_zeros (TensorFlowOp [(None, 100)]        0           tf_op_layer_zeros/packed[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_zeros_1 (TensorFlow [(None, 400)]        0           tf_op_layer_zeros_1/packed[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "loung (Loung)                   ((None, 1, 400), (No 4963801     tf_op_layer_zeros[0][0]          \n",
      "                                                                 embedding_1[1][0]                \n",
      "                                                                 embedding_1[2][0]                \n",
      "                                                                 embedding_1[3][0]                \n",
      "                                                                 embedding_1[4][0]                \n",
      "                                                                 embedding_1[5][0]                \n",
      "                                                                 embedding_1[6][0]                \n",
      "                                                                 embedding_1[7][0]                \n",
      "                                                                 embedding_1[8][0]                \n",
      "                                                                 embedding_1[9][0]                \n",
      "                                                                 embedding_1[10][0]               \n",
      "                                                                 embedding_1[11][0]               \n",
      "                                                                 embedding_1[12][0]               \n",
      "                                                                 embedding_1[13][0]               \n",
      "                                                                 embedding_1[14][0]               \n",
      "                                                                 embedding_1[15][0]               \n",
      "                                                                 embedding_1[16][0]               \n",
      "                                                                 embedding_1[17][0]               \n",
      "                                                                 embedding_1[18][0]               \n",
      "                                                                 embedding_1[19][0]               \n",
      "                                                                 embedding_1[20][0]               \n",
      "                                                                 embedding_1[21][0]               \n",
      "                                                                 embedding_1[22][0]               \n",
      "                                                                 embedding_1[23][0]               \n",
      "                                                                 embedding_1[24][0]               \n",
      "                                                                 embedding_1[25][0]               \n",
      "                                                                 embedding_1[26][0]               \n",
      "                                                                 embedding_1[27][0]               \n",
      "                                                                 embedding_1[28][0]               \n",
      "                                                                 embedding_1[29][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 5302)      2126102     loung[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax (TensorFlowO [(None, 1)]          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         multiple             530200      tf_op_layer_ArgMax[0][0]         \n",
      "                                                                 tf_op_layer_ArgMax_1[0][0]       \n",
      "                                                                 tf_op_layer_ArgMax_2[0][0]       \n",
      "                                                                 tf_op_layer_ArgMax_3[0][0]       \n",
      "                                                                 tf_op_layer_ArgMax_4[0][0]       \n",
      "                                                                 tf_op_layer_ArgMax_5[0][0]       \n",
      "                                                                 tf_op_layer_ArgMax_6[0][0]       \n",
      "                                                                 tf_op_layer_ArgMax_7[0][0]       \n",
      "                                                                 tf_op_layer_ArgMax_8[0][0]       \n",
      "                                                                 tf_op_layer_ArgMax_9[0][0]       \n",
      "                                                                 tf_op_layer_ArgMax_10[0][0]      \n",
      "                                                                 tf_op_layer_ArgMax_11[0][0]      \n",
      "                                                                 tf_op_layer_ArgMax_12[0][0]      \n",
      "                                                                 tf_op_layer_ArgMax_13[0][0]      \n",
      "                                                                 tf_op_layer_ArgMax_14[0][0]      \n",
      "                                                                 tf_op_layer_ArgMax_15[0][0]      \n",
      "                                                                 tf_op_layer_ArgMax_16[0][0]      \n",
      "                                                                 tf_op_layer_ArgMax_17[0][0]      \n",
      "                                                                 tf_op_layer_ArgMax_18[0][0]      \n",
      "                                                                 tf_op_layer_ArgMax_19[0][0]      \n",
      "                                                                 tf_op_layer_ArgMax_20[0][0]      \n",
      "                                                                 tf_op_layer_ArgMax_21[0][0]      \n",
      "                                                                 tf_op_layer_ArgMax_22[0][0]      \n",
      "                                                                 tf_op_layer_ArgMax_23[0][0]      \n",
      "                                                                 tf_op_layer_ArgMax_24[0][0]      \n",
      "                                                                 tf_op_layer_ArgMax_25[0][0]      \n",
      "                                                                 tf_op_layer_ArgMax_26[0][0]      \n",
      "                                                                 tf_op_layer_ArgMax_27[0][0]      \n",
      "                                                                 tf_op_layer_ArgMax_28[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1, 5302)      2126102     loung[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_1 (TensorFlo [(None, 1)]          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1, 5302)      2126102     loung[2][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_2 (TensorFlo [(None, 1)]          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1, 5302)      2126102     loung[3][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_3 (TensorFlo [(None, 1)]          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1, 5302)      2126102     loung[4][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_4 (TensorFlo [(None, 1)]          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1, 5302)      2126102     loung[5][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_5 (TensorFlo [(None, 1)]          0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1, 5302)      2126102     loung[6][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_6 (TensorFlo [(None, 1)]          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1, 5302)      2126102     loung[7][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_7 (TensorFlo [(None, 1)]          0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1, 5302)      2126102     loung[8][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_8 (TensorFlo [(None, 1)]          0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1, 5302)      2126102     loung[9][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_9 (TensorFlo [(None, 1)]          0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1, 5302)      2126102     loung[10][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_10 (TensorFl [(None, 1)]          0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1, 5302)      2126102     loung[11][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_11 (TensorFl [(None, 1)]          0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1, 5302)      2126102     loung[12][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_12 (TensorFl [(None, 1)]          0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 1, 5302)      2126102     loung[13][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_13 (TensorFl [(None, 1)]          0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 1, 5302)      2126102     loung[14][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_14 (TensorFl [(None, 1)]          0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 1, 5302)      2126102     loung[15][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_15 (TensorFl [(None, 1)]          0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 1, 5302)      2126102     loung[16][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_16 (TensorFl [(None, 1)]          0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 1, 5302)      2126102     loung[17][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_17 (TensorFl [(None, 1)]          0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 1, 5302)      2126102     loung[18][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_18 (TensorFl [(None, 1)]          0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 1, 5302)      2126102     loung[19][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_19 (TensorFl [(None, 1)]          0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 1, 5302)      2126102     loung[20][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_20 (TensorFl [(None, 1)]          0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 1, 5302)      2126102     loung[21][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_21 (TensorFl [(None, 1)]          0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 1, 5302)      2126102     loung[22][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_22 (TensorFl [(None, 1)]          0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 1, 5302)      2126102     loung[23][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_23 (TensorFl [(None, 1)]          0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 1, 5302)      2126102     loung[24][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_24 (TensorFl [(None, 1)]          0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 1, 5302)      2126102     loung[25][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_25 (TensorFl [(None, 1)]          0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 1, 5302)      2126102     loung[26][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_26 (TensorFl [(None, 1)]          0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 1, 5302)      2126102     loung[27][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_27 (TensorFl [(None, 1)]          0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 1, 5302)      2126102     loung[28][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ArgMax_28 (TensorFl [(None, 1)]          0           dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 1, 5302)      2126102     loung[29][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 30, 1, 5302)  0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "                                                                 dense_7[0][0]                    \n",
      "                                                                 dense_8[0][0]                    \n",
      "                                                                 dense_9[0][0]                    \n",
      "                                                                 dense_10[0][0]                   \n",
      "                                                                 dense_11[0][0]                   \n",
      "                                                                 dense_12[0][0]                   \n",
      "                                                                 dense_13[0][0]                   \n",
      "                                                                 dense_14[0][0]                   \n",
      "                                                                 dense_15[0][0]                   \n",
      "                                                                 dense_16[0][0]                   \n",
      "                                                                 dense_17[0][0]                   \n",
      "                                                                 dense_18[0][0]                   \n",
      "                                                                 dense_19[0][0]                   \n",
      "                                                                 dense_20[0][0]                   \n",
      "                                                                 dense_21[0][0]                   \n",
      "                                                                 dense_22[0][0]                   \n",
      "                                                                 dense_23[0][0]                   \n",
      "                                                                 dense_24[0][0]                   \n",
      "                                                                 dense_25[0][0]                   \n",
      "                                                                 dense_26[0][0]                   \n",
      "                                                                 dense_27[0][0]                   \n",
      "                                                                 dense_28[0][0]                   \n",
      "                                                                 dense_29[0][0]                   \n",
      "                                                                 dense_30[0][0]                   \n",
      "                                                                 dense_31[0][0]                   \n",
      "                                                                 dense_32[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 71,281,461\n",
      "Trainable params: 70,350,061\n",
      "Non-trainable params: 931,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 210334 samples, validate on 23371 samples\n",
      "Epoch 1/500\n",
      "210334/210334 [==============================] - 791s 4ms/sample - loss: 2.8785 - val_loss: 3.1965\n",
      "Epoch 2/500\n",
      "210334/210334 [==============================] - 802s 4ms/sample - loss: 2.5892 - val_loss: 3.0894\n",
      "Epoch 3/500\n",
      "210334/210334 [==============================] - 809s 4ms/sample - loss: 2.4924 - val_loss: 3.0294\n",
      "Epoch 4/500\n",
      "210334/210334 [==============================] - 814s 4ms/sample - loss: 2.4347 - val_loss: 2.9884\n",
      "Epoch 5/500\n",
      "210334/210334 [==============================] - 815s 4ms/sample - loss: 2.3949 - val_loss: 2.9577\n",
      "Epoch 6/500\n",
      "210334/210334 [==============================] - 812s 4ms/sample - loss: 2.3653 - val_loss: 2.9353\n",
      "Epoch 7/500\n",
      "210334/210334 [==============================] - 818s 4ms/sample - loss: 2.3418 - val_loss: 2.9194\n",
      "Epoch 8/500\n",
      "210334/210334 [==============================] - 814s 4ms/sample - loss: 2.3227 - val_loss: 2.9032\n",
      "Epoch 9/500\n",
      "210334/210334 [==============================] - 801s 4ms/sample - loss: 2.3067 - val_loss: 2.8904\n",
      "Epoch 10/500\n",
      "210334/210334 [==============================] - 802s 4ms/sample - loss: 2.2933 - val_loss: 2.8801\n",
      "Epoch 11/500\n",
      "210334/210334 [==============================] - 804s 4ms/sample - loss: 2.2815 - val_loss: 2.8710\n",
      "Epoch 12/500\n",
      "210334/210334 [==============================] - 813s 4ms/sample - loss: 2.2710 - val_loss: 2.8616\n",
      "Epoch 13/500\n",
      "210334/210334 [==============================] - 804s 4ms/sample - loss: 2.2615 - val_loss: 2.8543\n",
      "Epoch 14/500\n",
      " 95640/210334 [============>.................] - ETA: 7:01 - loss: 2.2593"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-9d05458e14c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model.fit([X, Y], Y, epochs=500, validation_split=0.1,batch_size=60,\n\u001b[0;32m----> 6\u001b[0;31m           \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbackss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m           \u001b[0;31m#, class_weight=class_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0;31m#,shuffle=False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3439\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3441\u001b[0;31m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3442\u001b[0m     \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m     \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m(op_input_list)\u001b[0m\n\u001b[1;32m    484\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    894\u001b[0m   \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m   \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    897\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m       \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/object_identity.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m       \u001b[0munwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0munwrapped\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/object_identity.py\u001b[0m in \u001b[0;36munwrapped\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0munwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#istraining=np.ones((X.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "model.fit([X, Y], Y, epochs=500, validation_split=0.1,batch_size=60,\n",
    "          callbacks = callbackss\n",
    "          #, class_weight=class_weights\n",
    "          #,shuffle=False\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():\n",
    "    inputs, targets, learning_rate, source_sequence_len, target_sequence_len, _ = model_inputs()\n",
    "    \n",
    "    max_target_sequence_len = 25\n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(inputs, [-1]),\n",
    "                                                  targets,\n",
    "                                                  batch_size,\n",
    "                                                  source_sequence_len,\n",
    "                                                  target_sequence_len,\n",
    "                                                  max_target_sequence_len,\n",
    "                                                  len(source_vocab_to_int),\n",
    "                                                  len(target_vocab_to_int),\n",
    "                                                  encoder_embedding_size,\n",
    "                                                  decoder_embedding_size,\n",
    "                                                  rnn_size,\n",
    "                                                  rnn_num_layers,\n",
    "                                                  target_vocab_to_int)\n",
    "    \n",
    "    training_logits = tf.identity(train_logits.rnn_output, name=\"logits\")\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name=\"predictions\")\n",
    "    \n",
    "    masks = tf.sequence_mask(target_sequence_len, max_target_sequence_len, dtype=tf.float32, name=\"masks\")\n",
    "    \n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        \n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        clipped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(clipped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_text.split(\"\\n\")[:51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "istrainingx=np.asarray([1])\n",
    "\n",
    "yy=np.expand_dims(Y[50], axis=0)\n",
    "print(Y[50])\n",
    "\n",
    "def make_prediction(sentence):\n",
    "\n",
    "    unk_idx = source_vocab_to_int[\"<UNK>\"]\n",
    "    word_idx=[]\n",
    "    for word in sentence.lower().split():\n",
    "        \n",
    "\n",
    "        word=source_vocab_to_int.get(word, unk_idx)\n",
    "        word_idx.append(word)\n",
    "    #word_idx = [source_vocab_to_int.get(word, unk_idx) for word in sentence.lower().split()]\n",
    "\n",
    "    word_idx = np.array(word_idx + [0] * (maxlength - len(word_idx)))\n",
    "    word_idx=word_idx[:maxlength]\n",
    "   # print(word_idx)\n",
    "\n",
    "    preds = model.predict([word_idx.reshape(-1,maxlength), yy, istrainingx])\n",
    "    predictions = np.argmax(preds, axis=-1)\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "    idx = [target_int_to_vocab.get(int(idx), \"<UNK>\") for idx in predictions[0]]\n",
    " \n",
    " \n",
    "    return \" \".join(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(source_int_to_vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_text.split(\"\\n\")[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number=8000\n",
    "print(source_text.split(\"\\n\")[number])\n",
    "make_prediction(source_text.split(\"\\n\")[number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_prediction('government')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_sentence_text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "translate_sentence = sentence_to_seq(translate_sentence_text, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph('checkpoints/dev.meta')\n",
    "    loader.restore(sess, tf.train.latest_checkpoint('./checkpoints'))\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('inputs:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_len:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_len:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n",
    "                                         source_sequence_length: [len(translate_sentence)]*batch_size})[0]\n",
    "\n",
    "print('【Input】')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\n【Prediction】')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
    "print('  French Words: {}'.format([target_int_to_vocab[i] for i in translate_logits]))\n",
    "\n",
    "print(\"\\n【Full Sentence】\")\n",
    "print(\" \".join([target_int_to_vocab[i] for i in translate_logits]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fr_preds = []\n",
    "\n",
    "\n",
    "for sentence in (source_text.split(\"\\n\")[:2000]):\n",
    "    fr_pred = make_prediction(sentence)\n",
    "   \n",
    "    fr_preds.append(fr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "references = target_text.split(\"\\n\")[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bleu_score = []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(fr_preds))):\n",
    " \n",
    "    pred = fr_preds[i].replace(\"<EOS>\", \"\").replace(\"<PAD>\", \"\").rstrip()\n",
    "    reference = references[i].lower()\n",
    "   \n",
    "    score = sentence_bleu([reference.split()], pred.split())\n",
    "    if(score<0.8):\n",
    "        print(pred)\n",
    "        print(reference)\n",
    "    bleu_score.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The BLEU score is  {}\".format(sum(bleu_score) / len(bleu_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
